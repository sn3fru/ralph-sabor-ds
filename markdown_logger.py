"""
M√≥dulo de Logging Markdown Multimodal para Credit Scoring
Gera relat√≥rios estruturados enriquecidos com an√°lise visual via LLM.
"""

import os
import base64
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Callable
import json
import matplotlib.pyplot as plt


# ==============================================================================
# üß† C√âREBRO VISUAL (Integra√ß√£o com LLM)
# ==============================================================================

def load_env_file(env_path: Path) -> dict:
    """
    Carrega vari√°veis de um arquivo .env.
    
    Args:
        env_path: Caminho para o arquivo .env
    
    Returns:
        Dicion√°rio com as vari√°veis do .env
    """
    env_vars = {}
    if env_path.exists():
        try:
            with open(env_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        key = key.strip()
                        value = value.strip().strip('"').strip("'")  # Remove aspas
                        env_vars[key] = value
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao ler arquivo .env: {e}")
    return env_vars


class VisionAnalyst:
    """Classe respons√°vel por enviar imagens para uma LLM e obter an√°lises."""
    
    def __init__(self, api_key: str = None, provider: str = "openai", model_name: str = None):
        """
        Inicializa o analista visual.
        
        Args:
            api_key: Chave da API (se None, tenta pegar do ambiente ou .env)
            provider: Provedor a usar ('openai', 'gemini', 'claude')
            model_name: Nome do modelo a usar (especialmente para Gemini, l√™ do .env se None)
        """
        # Tentar carregar do .env primeiro
        env_path = Path(__file__).parent / '.env'
        env_vars = load_env_file(env_path)
        
        # Determinar API key: par√¢metro > .env > vari√°vel de ambiente
        if api_key:
            self.api_key = api_key
        elif provider.lower() == "gemini" and "GEMINI_KEY" in env_vars:
            self.api_key = env_vars["GEMINI_KEY"]
            print(f"   ‚úÖ API Key carregada do arquivo .env")
        else:
            self.api_key = (
                os.getenv("OPENAI_API_KEY") or 
                os.getenv("GEMINI_API_KEY") or 
                os.getenv("ANTHROPIC_API_KEY")
            )
        
        self.provider = provider.lower()
        
        # Determinar model_name: par√¢metro > .env > default
        if model_name:
            self.model_name = model_name
        elif provider.lower() == "gemini" and "MODEL_NAME" in env_vars:
            self.model_name = env_vars["MODEL_NAME"]
            print(f"   ‚úÖ Modelo carregado do arquivo .env: {self.model_name}")
        elif provider.lower() == "gemini":
            self.model_name = "gemini-1.5-pro"  # Default
        elif provider.lower() == "openai":
            self.model_name = "gpt-4o"
        elif provider.lower() == "claude":
            self.model_name = "claude-3-5-sonnet-20241022"
        else:
            self.model_name = None
        
        self._client = None
    
    def _get_client(self):
        """Lazy loading do cliente da API."""
        if self._client is not None:
            return self._client
        
        if not self.api_key:
            return None
        
        try:
            if self.provider == "openai":
                try:
                    from openai import OpenAI
                    self._client = OpenAI(api_key=self.api_key)
                    return self._client
                except ImportError:
                    print("‚ö†Ô∏è Biblioteca 'openai' n√£o instalada. Instale com: pip install openai")
                    return None
            
            elif self.provider == "gemini":
                try:
                    import google.generativeai as genai
                    genai.configure(api_key=self.api_key)
                    # Usa o model_name do .env ou par√¢metro, sen√£o usa default
                    model_to_use = self.model_name or "gemini-1.5-pro"
                    self._client = genai.GenerativeModel(model_to_use)
                    print(f"   üìå Usando modelo Gemini: {model_to_use}")
                    return self._client
                except ImportError:
                    print("‚ö†Ô∏è Biblioteca 'google-generativeai' n√£o instalada. Instale com: pip install google-generativeai")
                    return None
            
            elif self.provider == "claude":
                try:
                    from anthropic import Anthropic
                    self._client = Anthropic(api_key=self.api_key)
                    return self._client
                except ImportError:
                    print("‚ö†Ô∏è Biblioteca 'anthropic' n√£o instalada. Instale com: pip install anthropic")
                    return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao inicializar cliente {self.provider}: {e}")
            return None
    
    def analyze_image(self, image_path: str, local_context: str, global_context: Dict = None) -> str:
        """
        Envia a imagem para a LLM e retorna a an√°lise textual.
        Agora com suporte a contexto global para evitar alucina√ß√µes.
        
        Args:
            image_path: Caminho para a imagem
            local_context: Contexto t√©cnico espec√≠fico deste gr√°fico
            global_context: Dicion√°rio com contexto global do estudo (dataset, m√©tricas, etc)
        
        Returns:
            An√°lise textual da imagem
        """
        if not self.api_key:
            return (
                "> *[IA Vision - Modo Simula√ß√£o]*: API Key n√£o configurada. "
                "Configure GEMINI_KEY no arquivo .env ou vari√°veis de ambiente para an√°lise visual autom√°tica. "
                "O gr√°fico foi salvo e pode ser analisado manualmente."
            )
        
        client = self._get_client()
        if not client:
            return (
                "> *[IA Vision - Erro]*: N√£o foi poss√≠vel inicializar o cliente da API. "
                "Verifique se a biblioteca do provider est√° instalada e a API key est√° correta."
            )
        
        try:
            # Codificar imagem em base64
            with open(image_path, "rb") as image_file:
                encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
            
            # Prompt Engineering Hier√°rquico Aprimorado
            system_prompt = (
                "Voc√™ √© um Cientista de Dados S√™nior (Head de Risco) especialista em Credit Scoring. "
                "Sua fun√ß√£o √© validar visualmente os dados num√©ricos fornecidos e gerar insights acion√°veis.\n\n"
                "REGRAS DE OURO:\n"
                "1. O 'Contexto Global' cont√©m a verdade sobre os dados (tamanhos, balanceamento, m√©tricas anteriores). "
                "Use isso para contextualizar sua an√°lise.\n"
                "2. O 'Contexto Local' cont√©m c√°lculos geom√©tricos da curva. "
                "Se o c√°lculo parecer contradizer a imagem (ex: texto diz 'slope 0' mas imagem mostra subida), "
                "APONTE A DISCREP√ÇNCIA e confie na imagem para a an√°lise qualitativa.\n"
                "3. Seja c√©tico e cr√≠tico. Se o AUC √© alto mas a curva parece ruim, alerte. "
                "Se h√° contradi√ß√£o entre texto e imagem, identifique e explique.\n"
                "4. Conecte os pontos: Use o contexto global (ex: desbalanceamento severo) para explicar "
                "comportamentos observados no gr√°fico.\n"
                "5. Seja t√©cnico mas pr√°tico: Foque em geometria da curva, tend√™ncias, anomalias, "
                "sinais de overfitting, e conclus√µes para tomada de decis√£o."
            )
            
            # Formatar contexto global como JSON leg√≠vel
            if global_context and len(global_context) > 0:
                global_summary = json.dumps(global_context, indent=2, ensure_ascii=False)
                global_section = f"--- CONTEXTO GLOBAL DO ESTUDO ---\n{global_summary}\n\n"
            else:
                global_section = ""
            
            user_prompt = (
                f"{global_section}"
                f"--- CONTEXTO T√âCNICO DESTE GR√ÅFICO ---\n{local_context}\n\n"
                f"--- TAREFA ---\n"
                "Analise a imagem anexa. Valide se a geometria visual condiz com os n√∫meros fornecidos. "
                "Se houver contradi√ß√£o entre texto e imagem, identifique e explique. "
                "Forne√ßa:\n"
                "1. Observa√ß√µes sobre a forma/geometria da curva (validando contra o contexto local)\n"
                "2. Sinais de problemas (overfitting, drift, etc) considerando o contexto global\n"
                "3. Conclus√µes pr√°ticas para o neg√≥cio conectando contexto global + visual\n"
                "Seja conciso mas completo (m√°ximo 250 palavras)."
            )
            
            # Chamada para OpenAI GPT-4o
            if self.provider == "openai":
                response = client.chat.completions.create(
                    model=self.model_name or "gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": [
                            {"type": "text", "text": user_prompt},
                            {"type": "image_url", "image_url": {
                                "url": f"data:image/png;base64,{encoded_string}",
                                "detail": "high"
                            }}
                        ]}
                    ],
                    max_tokens=400,
                    temperature=0.3
                )
                return response.choices[0].message.content
            
            # Chamada para Google Gemini
            elif self.provider == "gemini":
                import PIL.Image
                image = PIL.Image.open(image_path)
                # O cliente j√° foi inicializado com o model_name correto em _get_client()
                # N√£o precisa especificar novamente aqui
                # Gemini recebe system + user prompt combinados
                full_prompt = system_prompt + "\n\n" + user_prompt
                response = client.generate_content([full_prompt, image])
                return response.text
            
            # Chamada para Anthropic Claude
            elif self.provider == "claude":
                with open(image_path, "rb") as image_file:
                    image_data = image_file.read()
                
                message = client.messages.create(
                    model=self.model_name or "claude-3-5-sonnet-20241022",
                    max_tokens=400,
                    system=system_prompt,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {"type": "image", "source": {
                                    "type": "base64",
                                    "media_type": "image/png",
                                    "data": encoded_string
                                }},
                                {"type": "text", "text": user_prompt}
                            ]
                        }
                    ]
                )
                return message.content[0].text
            
            else:
                return f"> *[Erro]*: Provider '{self.provider}' n√£o implementado."
        
        except Exception as e:
            error_msg = str(e)
            return (
                f"> *[Erro na An√°lise Visual]*: {error_msg}\n"
                "> O gr√°fico foi salvo mas a an√°lise autom√°tica falhou. "
                "Verifique sua API key e conex√£o com a internet."
            )
    
    def text_inference(self, system_prompt: str, user_prompt: str) -> str:
        """
        M√©todo para infer√™ncia de texto puro (sem imagem).
        Usado pelo meta_controller para decis√µes baseadas em relat√≥rios.
        
        Args:
            system_prompt: Prompt do sistema
            user_prompt: Prompt do usu√°rio
        
        Returns:
            Resposta textual da LLM
        """
        if not self.api_key:
            return json.dumps({
                "decision": "STOP",
                "reasoning": "API Key n√£o configurada. N√£o √© poss√≠vel fazer infer√™ncia.",
                "changes": {}
            })
        
        client = self._get_client()
        if not client:
            return json.dumps({
                "decision": "STOP",
                "reasoning": "Cliente LLM n√£o inicializado.",
                "changes": {}
            })
        
        try:
            # Chamada para OpenAI GPT-4o
            if self.provider == "openai":
                response = client.chat.completions.create(
                    model=self.model_name or "gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=1000,
                    temperature=0.3,
                    response_format={"type": "json_object"}  # For√ßa resposta JSON
                )
                return response.choices[0].message.content
            
            # Chamada para Google Gemini
            elif self.provider == "gemini":
                # Gemini precisa de instru√ß√£o expl√≠cita para JSON
                json_prompt = f"{system_prompt}\n\nIMPORTANTE: Responda APENAS com JSON v√°lido, sem markdown, sem texto adicional.\n\n{user_prompt}"
                response = client.generate_content(json_prompt)
                text = response.text.strip()
                # Remove markdown code blocks se houver
                if text.startswith("```json"):
                    text = text.replace("```json", "").replace("```", "").strip()
                elif text.startswith("```"):
                    text = text.replace("```", "").strip()
                return text
            
            # Chamada para Anthropic Claude
            elif self.provider == "claude":
                message = client.messages.create(
                    model=self.model_name or "claude-3-5-sonnet-20241022",
                    max_tokens=1000,
                    system=system_prompt,
                    messages=[
                        {
                            "role": "user",
                            "content": f"{user_prompt}\n\nIMPORTANTE: Responda APENAS com JSON v√°lido, sem markdown."
                        }
                    ]
                )
                text = message.content[0].text.strip()
                # Remove markdown code blocks se houver
                if text.startswith("```json"):
                    text = text.replace("```json", "").replace("```", "").strip()
                elif text.startswith("```"):
                    text = text.replace("```", "").strip()
                return text
            
            else:
                return json.dumps({
                    "decision": "STOP",
                    "reasoning": f"Provider '{self.provider}' n√£o implementado para text_inference.",
                    "changes": {}
                })
        
        except Exception as e:
            error_msg = str(e)
            return json.dumps({
                "decision": "STOP",
                "reasoning": f"Erro na infer√™ncia LLM: {error_msg}",
                "changes": {}
            })


# ==============================================================================
# üìù LOGGER PRINCIPAL
# ==============================================================================

class MarkdownLogger:
    """
    Logger que gera relat√≥rios markdown estruturados para an√°lise por LLMs.
    Substitui prints e displays do notebook por logging estruturado.
    """
    
    def __init__(self, output_dir: str = "reports", run_name: Optional[str] = None, 
                 use_vision_llm: bool = False, vision_provider: str = "openai", 
                 vision_api_key: str = None, vision_model_name: str = None):
        """
        Inicializa o logger markdown.
        
        Args:
            output_dir: Diret√≥rio onde salvar os relat√≥rios
            run_name: Nome da execu√ß√£o (se None, usa timestamp)
            use_vision_llm: Se True, ativa an√°lise visual autom√°tica de gr√°ficos
            vision_provider: Provedor de LLM ('openai', 'gemini', 'claude')
            vision_api_key: Chave da API (se None, tenta pegar do ambiente ou .env)
            vision_model_name: Nome do modelo (especialmente para Gemini, l√™ do .env se None)
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        if run_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            run_name = f"credit_scoring_{timestamp}"
        
        self.run_name = run_name
        self.report_path = self.output_dir / f"{run_name}.md"
        self.images_dir = self.output_dir / f"{run_name}_images"
        self.images_dir.mkdir(exist_ok=True)
        
        self.sections = []
        self.current_section = None
        self.image_counter = 0
        
        # ‚úÖ MEM√ìRIA DE CONTEXTO GLOBAL (Global Context Memory)
        self.global_context = {}  # Armazena fatos importantes do pipeline
        
        # Inicializa o analista visual se solicitado
        self.vision_analyst = None
        if use_vision_llm:
            self.vision_analyst = VisionAnalyst(
                api_key=vision_api_key, 
                provider=vision_provider,
                model_name=vision_model_name
            )
            model_info = f" ({self.vision_analyst.model_name})" if self.vision_analyst.model_name else ""
            print(f"‚úÖ An√°lise Visual ativada (Provider: {vision_provider}{model_info})")
        
        # Inicializar relat√≥rio
        self._init_report()
    
    def update_context(self, key: str, value: Any):
        """
        Atualiza a mem√≥ria global do logger com fatos importantes do pipeline.
        Este contexto ser√° injetado automaticamente em todas as an√°lises visuais.
        
        Args:
            key: Chave do contexto (ex: 'class_balance_ratio', 'n_features')
            value: Valor do contexto (ser√° formatado automaticamente se for float)
        """
        # Se for float, formata para n√£o poluir o JSON e evitar precis√£o excessiva
        if isinstance(value, float):
            value = round(value, 4)
        elif isinstance(value, (int, str, bool)):
            value = value
        else:
            # Para outros tipos, converte para string
            value = str(value)
        
        self.global_context[key] = value
    
    def _init_report(self):
        """Inicializa o arquivo markdown com cabe√ßalho."""
        vision_status = "Ativado" if self.vision_analyst else "Desativado"
        vision_provider = self.vision_analyst.provider if self.vision_analyst else "N/A"
        
        header = f"""# üìä Relat√≥rio de Execu√ß√£o: Credit Scoring Model

**Data/Hora:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Execu√ß√£o:** `{self.run_name}`
**Modo Vision:** `{vision_status}` ({vision_provider})

---

"""
        with open(self.report_path, 'w', encoding='utf-8') as f:
            f.write(header)
    
    def section(self, title: str, level: int = 2):
        """
        Inicia uma nova se√ß√£o no relat√≥rio.
        
        Args:
            title: T√≠tulo da se√ß√£o
            level: N√≠vel do cabe√ßalho (2 = ##, 3 = ###, etc)
        """
        self.current_section = {
            'title': title,
            'level': level,
            'content': [],
            'images': [],
            'metrics': {},
            'insights': []
        }
        self.sections.append(self.current_section)
        
        prefix = '#' * level
        with open(self.report_path, 'a', encoding='utf-8') as f:
            f.write(f"\n{prefix} {title}\n\n")
    
    def log(self, message: str, level: str = "info"):
        """
        Adiciona uma mensagem ao relat√≥rio.
        
        Args:
            message: Mensagem a ser logada
            level: N√≠vel da mensagem (info, warning, error, success)
        """
        if self.current_section is None:
            self.section("Log Geral", level=2)
        
        # Emojis por n√≠vel
        emoji_map = {
            'info': '‚ÑπÔ∏è',
            'warning': '‚ö†Ô∏è',
            'error': '‚ùå',
            'success': '‚úÖ',
            'critical': 'üö®'
        }
        
        emoji = emoji_map.get(level, '‚ÑπÔ∏è')
        formatted_message = f"{emoji} {message}"
        
        self.current_section['content'].append(formatted_message)
        
        with open(self.report_path, 'a', encoding='utf-8') as f:
            f.write(f"{formatted_message}\n\n")
    
    def _format_value(self, value: Any) -> str:
        """
        Formata valores de forma segura para Markdown.
        Remove cifr√£o R$ para evitar conflito com LaTeX.
        
        Args:
            value: Valor a ser formatado
        
        Returns:
            String formatada
        """
        if isinstance(value, float):
            formatted = f"{value:.4f}"
        elif isinstance(value, (int, str)):
            formatted = str(value)
        else:
            formatted = str(value)
        
        # ‚úÖ Remover R$ para evitar conflito com LaTeX no Markdown
        # Substituir por BRL ou apenas remover o prefixo
        if "R$" in formatted:
            formatted = formatted.replace("R$", "").strip()
            # Tamb√©m remover vers√£o escapada se existir (usando raw string para evitar SyntaxWarning)
            formatted = formatted.replace(r"R\$", "").strip()
            # Se houver valor num√©rico, manter apenas o n√∫mero
            # Ex: "R$ 1.000,00" -> "1.000,00" ou "1000.00"
        
        return formatted
    
    def log_metric(self, name: str, value: Any, description: Optional[str] = None):
        """
        Registra uma m√©trica no relat√≥rio.
        
        Args:
            name: Nome da m√©trica
            value: Valor da m√©trica
            description: Descri√ß√£o opcional
        """
        if self.current_section is None:
            self.section("M√©tricas", level=2)
        
        # ‚úÖ Usar formata√ß√£o segura
        formatted_value = self._format_value(value)
        
        self.current_section['metrics'][name] = {
            'value': formatted_value,
            'raw': value,
            'description': description
        }
        
        # Escrever no markdown
        with open(self.report_path, 'a', encoding='utf-8') as f:
            f.write(f"**{name}:** {formatted_value}")
            if description:
                f.write(f" - {description}")
            f.write("\n\n")
    
    def log_table(self, title: str, data: Dict[str, Any] or List[Dict], headers: Optional[List[str]] = None):
        """
        Registra uma tabela no relat√≥rio.
        
        Args:
            title: T√≠tulo da tabela
            data: Dados da tabela (dict ou lista de dicts)
            headers: Cabe√ßalhos opcionais
        """
        if self.current_section is None:
            self.section("Tabelas", level=2)
        
        with open(self.report_path, 'a', encoding='utf-8') as f:
            f.write(f"### {title}\n\n")
            
            if isinstance(data, dict):
                # Tabela de chave-valor
                f.write("| M√©trica | Valor |\n")
                f.write("|---------|-------|\n")
                for key, value in data.items():
                    # ‚úÖ Usar formata√ß√£o segura (remove R$)
                    formatted_value = self._format_value(value)
                    f.write(f"| {key} | {formatted_value} |\n")
            elif isinstance(data, list) and len(data) > 0:
                # Tabela de m√∫ltiplas linhas
                if headers is None:
                    headers = list(data[0].keys())
                
                f.write("| " + " | ".join(headers) + " |\n")
                f.write("|" + "|".join(["---"] * len(headers)) + "|\n")
                
                for row in data[:20]:  # Limitar a 20 linhas
                    values = [str(row.get(h, "")) for h in headers]
                    f.write("| " + " | ".join(values) + " |\n")
                
                if len(data) > 20:
                    f.write(f"\n*Mostrando 20 de {len(data)} linhas*\n")
            
            f.write("\n")
    
    def log_insight(self, insight: str, category: str = "geral"):
        """
        Registra um insight ou conclus√£o importante.
        
        Args:
            insight: Texto do insight
            category: Categoria do insight (geral, overfitting, drift, financeiro, etc)
        """
        if self.current_section is None:
            self.section("Insights", level=2)
        
        self.current_section['insights'].append({
            'text': insight,
            'category': category
        })
        
        with open(self.report_path, 'a', encoding='utf-8') as f:
            f.write(f"**üí° Insight ({category}):** {insight}\n\n")
    
    def log_plot(self, fig, description: str, save_image: bool = True, 
                 title: Optional[str] = None, context_description: Optional[str] = None,
                 analyze: bool = True):
        """
        Salva um gr√°fico e adiciona descri√ß√£o textual ao relat√≥rio.
        Opcionalmente analisa o gr√°fico com IA Vision se ativado.
        
        Args:
            fig: Figura matplotlib
            description: Descri√ß√£o textual do gr√°fico (legado, mantido para compatibilidade)
            save_image: Se True, salva a imagem
            title: T√≠tulo do gr√°fico (se None, usa descri√ß√£o)
            context_description: Contexto t√©cnico detalhado para an√°lise IA (recomendado)
            analyze: Se True e vision_analyst ativo, chama an√°lise visual autom√°tica
        """
        if self.current_section is None:
            self.section("Visualiza√ß√µes", level=2)
        
        self.image_counter += 1
        image_filename = f"img_{self.image_counter:03d}_{self.run_name}.png"
        image_path = self.images_dir / image_filename
        
        # Usar title se fornecido, sen√£o usar primeira parte da description
        plot_title = title or description.split('.')[0] if description else f"Gr√°fico {self.image_counter}"
        
        # Contexto t√©cnico: usar context_description se fornecido, sen√£o description
        technical_context = context_description or description or "Gr√°fico gerado durante an√°lise de credit scoring."
        
        if save_image:
            fig.savefig(image_path, dpi=100, bbox_inches='tight')
            plt.close(fig)  # Fecha para liberar mem√≥ria
        
        self.current_section['images'].append({
            'path': str(image_path),
            'description': description or technical_context,
            'filename': image_filename,
            'analyzed': analyze and self.vision_analyst is not None
        })
        
        # Preparar conte√∫do para escrita (evita erro de I/O em arquivo fechado)
        content_to_write = f"### {plot_title}\n\n"
        content_to_write += f"![{plot_title}]({self.images_dir.name}/{image_filename})\n\n"
        content_to_write += f"**Contexto T√©cnico:** {technical_context}\n\n"
        
        # üß† AN√ÅLISE VISUAL AUTOM√ÅTICA (Processamento antes de escrever)
        # ‚úÖ Agora passa o contexto global para evitar alucina√ß√µes
        ai_analysis_text = ""
        if analyze and self.vision_analyst:
            print(f"   üëÅÔ∏è Analisando gr√°fico '{plot_title}' com IA Vision ({self.vision_analyst.provider})...")
            try:
                ai_response = self.vision_analyst.analyze_image(
                    str(image_path), 
                    technical_context,
                    self.global_context  # ‚úÖ INJE√á√ÉO DO CONTEXTO GLOBAL
                )
                # Formata a an√°lise com blockquote para ficar bonito no markdown
                formatted_analysis = ai_response.replace('\n', '\n> ')
                ai_analysis_text = f"> ü§ñ **An√°lise Visual Autom√°tica:**\n>\n> {formatted_analysis}\n\n"
            except Exception as e:
                ai_analysis_text = f"> ‚ö†Ô∏è **Erro na an√°lise visual:** {str(e)}\n\n"
        elif analyze and not self.vision_analyst:
            ai_analysis_text = f"> ‚ÑπÔ∏è *An√°lise visual autom√°tica dispon√≠vel. Configure `use_vision_llm=True` e uma API key para ativar.*\n\n"
        
        # Escrita √∫nica no arquivo (evita erro de I/O em arquivo fechado)
        with open(self.report_path, 'a', encoding='utf-8') as f:
            f.write(content_to_write)
            if ai_analysis_text:
                f.write(ai_analysis_text)
    
    def log_plot_description(self, description: str, analysis: str):
        """
        Adiciona descri√ß√£o textual detalhada de um gr√°fico.
        
        Args:
            description: Descri√ß√£o do que o gr√°fico mostra
            analysis: An√°lise e conclus√µes do gr√°fico
        """
        with open(self.report_path, 'a', encoding='utf-8') as f:
            f.write(f"**An√°lise do Gr√°fico:**\n\n")
            f.write(f"{description}\n\n")
            f.write(f"**Conclus√µes:**\n\n")
            f.write(f"{analysis}\n\n")
    
    def log_code_block(self, code: str, language: str = "python"):
        """
        Adiciona um bloco de c√≥digo ao relat√≥rio.
        
        Args:
            code: C√≥digo a ser exibido
            language: Linguagem do c√≥digo
        """
        with open(self.report_path, 'a', encoding='utf-8') as f:
            f.write(f"```{language}\n{code}\n```\n\n")
    
    def log_summary(self, title: str, items: List[str]):
        """
        Adiciona um resumo em lista ao relat√≥rio.
        
        Args:
            title: T√≠tulo do resumo
            items: Lista de itens
        """
        with open(self.report_path, 'a', encoding='utf-8') as f:
            f.write(f"### {title}\n\n")
            for item in items:
                f.write(f"- {item}\n")
            f.write("\n")
    
    def log_parameters(self, params: Dict[str, Any], section_name: str = "Par√¢metros"):
        """
        Registra par√¢metros de configura√ß√£o.
        
        Args:
            params: Dicion√°rio de par√¢metros
            section_name: Nome da se√ß√£o
        """
        with open(self.report_path, 'a', encoding='utf-8') as f:
            f.write(f"### {section_name}\n\n")
            f.write("| Par√¢metro | Valor |\n")
            f.write("|-----------|-------|\n")
            for key, value in params.items():
                formatted_value = f"{value:.4f}" if isinstance(value, float) else str(value)
                f.write(f"| `{key}` | {formatted_value} |\n")
            f.write("\n")
    
    def describe_roc_curve(self, fpr: List[float], tpr: List[float], auc: float, 
                          threshold: Optional[float] = None) -> str:
        """
        Gera descri√ß√£o textual de uma curva ROC.
        
        Args:
            fpr: Taxa de falsos positivos
            tpr: Taxa de verdadeiros positivos
            auc: Valor AUC
            threshold: Threshold usado (opcional)
        
        Returns:
            Descri√ß√£o textual da curva
        """
        description = f"""
**An√°lise da Curva ROC:**

- **AUC-ROC:** {auc:.4f}
  - {'Excelente' if auc >= 0.9 else 'Bom' if auc >= 0.8 else 'Moderado' if auc >= 0.7 else 'Fraco'} poder discriminativo
  - O modelo consegue distinguir {'muito bem' if auc >= 0.9 else 'bem' if auc >= 0.8 else 'moderadamente'} entre bons e maus pagadores

- **Forma da Curva:**
  - A curva {'sobe rapidamente' if tpr[10] > 0.5 else 'sobe gradualmente'} no in√≠cio, indicando que o modelo identifica {'facilmente' if tpr[10] > 0.5 else 'gradualmente'} os casos de maior risco
  - {'Curva pr√≥xima do canto superior esquerdo' if auc >= 0.9 else 'Curva acima da diagonal'} indica boa separa√ß√£o de classes

"""
        if threshold:
            description += f"- **Threshold √ìtimo:** {threshold:.4f}\n"
            idx = min(range(len(fpr)), key=lambda i: abs(fpr[i] - (1 - threshold)))
            description += f"  - Neste ponto: TPR = {tpr[idx]:.4f}, FPR = {fpr[idx]:.4f}\n"
        
        return description
    
    def describe_distribution(self, data: List[float] or Any, name: str) -> str:
        """
        Gera descri√ß√£o textual de uma distribui√ß√£o.
        
        Args:
            data: Dados para an√°lise
            name: Nome da vari√°vel
        
        Returns:
            Descri√ß√£o textual
        """
        import numpy as np
        
        if isinstance(data, (list, np.ndarray)):
            arr = np.array(data)
            mean_val = np.mean(arr)
            std_val = np.std(arr)
            min_val = np.min(arr)
            max_val = np.max(arr)
            median_val = np.median(arr)
            
            description = f"""
**Distribui√ß√£o de {name}:**

- **Estat√≠sticas Descritivas:**
  - M√©dia: {mean_val:.4f}
  - Mediana: {median_val:.4f}
  - Desvio Padr√£o: {std_val:.4f}
  - M√≠nimo: {min_val:.4f}
  - M√°ximo: {max_val:.4f}

- **Interpreta√ß√£o:**
  - {'Distribui√ß√£o sim√©trica' if abs(mean_val - median_val) < 0.1 * std_val else 'Distribui√ß√£o assim√©trica'}
  - {'Baixa variabilidade' if std_val < 0.1 * abs(mean_val) else 'Alta variabilidade'} (CV = {std_val/abs(mean_val) if mean_val != 0 else 'N/A':.2f})
"""
            return description
        
        return f"**{name}:** {str(data)}\n"
    
    def log_dataframe_head(self, df, n=5, title="Amostra de Dados"):
        """
        Adiciona uma tabela markdown do head de um DataFrame.
        
        Args:
            df: DataFrame do pandas
            n: N√∫mero de linhas a mostrar
            title: T√≠tulo da tabela
        """
        if self.current_section is None:
            self.section("Dados", level=2)
        
        with open(self.report_path, 'a', encoding='utf-8') as f:
            f.write(f"### {title}\n\n")
            
            # Cabe√ßalho
            headers = list(df.columns)
            f.write("| " + " | ".join(headers) + " |\n")
            f.write("|" + "|".join(["---"] * len(headers)) + "|\n")
            
            # Linhas
            for idx, row in df.head(n).iterrows():
                values = [str(val)[:50] if len(str(val)) > 50 else str(val) for val in row]
                f.write("| " + " | ".join(values) + " |\n")
            
            if len(df) > n:
                f.write(f"\n*Mostrando {n} de {len(df)} linhas*\n")
            
            f.write("\n")
    
    def log_timestamp(self, step_name: str):
        """
        Adiciona timestamp para medir performance de cada etapa.
        
        Args:
            step_name: Nome da etapa
        """
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.log(f"[{timestamp}] {step_name}", "info")
    
    def finalize(self):
        """Finaliza o relat√≥rio adicionando resumo executivo."""
        summary = f"""
---

## üìã Resumo Executivo

**Data de Execu√ß√£o:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

### Se√ß√µes do Relat√≥rio:

"""
        for i, section in enumerate(self.sections, 1):
            summary += f"{i}. {section['title']}\n"
            if section['metrics']:
                summary += f"   - {len(section['metrics'])} m√©tricas registradas\n"
            if section['insights']:
                summary += f"   - {len(section['insights'])} insights identificados\n"
            if section['images']:
                summary += f"   - {len(section['images'])} visualiza√ß√µes geradas\n"
        
        summary += f"""
### Estat√≠sticas da Execu√ß√£o:

- **Total de Se√ß√µes:** {len(self.sections)}
- **Total de M√©tricas:** {sum(len(s['metrics']) for s in self.sections)}
- **Total de Insights:** {sum(len(s['insights']) for s in self.sections)}
- **Total de Visualiza√ß√µes:** {sum(len(s['images']) for s in self.sections)}

---

**Relat√≥rio gerado automaticamente pelo sistema de Credit Scoring**
"""
        
        with open(self.report_path, 'a', encoding='utf-8') as f:
            f.write(summary)
        
        print(f"‚úÖ Relat√≥rio markdown salvo em: {self.report_path}")
        print(f"üìÅ Imagens salvas em: {self.images_dir}")
        if self.vision_analyst:
            print(f"ü§ñ An√°lise visual: {self.image_counter} gr√°ficos processados")
