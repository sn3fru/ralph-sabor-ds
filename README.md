# ğŸ§¬ Ralph DS v2.0: Agente AutÃ´nomo de Data Science

![Status](https://img.shields.io/badge/Status-Experimental-orange) ![Python](https://img.shields.io/badge/Python-3.10+-blue) ![AI](https://img.shields.io/badge/Powered%20by-LLM%20%2B%20Vision-purple)

**Ralph DS** Ã© um agente autÃ´nomo **AGNÃ“STICO** que resolve qualquer problema de Data Science.

## Novidades da v2.0

- **AgnÃ³stico ao DomÃ­nio**: Detecta automaticamente o tipo de problema (classificaÃ§Ã£o, regressÃ£o, etc.)
- **EDA ObrigatÃ³ria**: Todo problema comeÃ§a com anÃ¡lise exploratÃ³ria
- **STATE.md**: MemÃ³ria resumida para nÃ£o reler todos os arquivos
- **Planejamento DinÃ¢mico**: TASK_LIST adaptativa baseada nos insights
- **context/data/**: Dados e contexto do projeto em um sÃ³ lugar

Diferente de pipelines de AutoML tradicionais que executam uma busca linear, o Ralph opera em um ciclo contÃ­nuo de raciocÃ­nio, codificaÃ§Ã£o, anÃ¡lise visual e tomada de decisÃ£o estratÃ©gica.

---

## ğŸ“– Filosofia e MotivaÃ§Ã£o

### O Problema da Linearidade

A Engenharia de Software tradicional Ã© determinÃ­stica: especifica-se a entrada e a saÃ­da, e constrÃ³i-se o caminho. A CiÃªncia de Dados Ã© **estocÃ¡stica e exploratÃ³ria**.

1.**O Feedback Loop:** Em DS, um erro nÃ£o Ã© apenas uma falha de execuÃ§Ã£o; Ã© uma informaÃ§Ã£o sobre a distribuiÃ§Ã£o dos dados. Um pipeline linear falha quando encontra um *drift* inesperado. Um agente precisa "parar, olhar e corrigir".

2.**A Arte da DecisÃ£o:** A escolha entre uma RegressÃ£o Bayesiana e uma Rede Neural nÃ£o Ã© apenas uma questÃ£o de acurÃ¡cia. Envolve interpretabilidade, custo computacional e apetite de risco. O Ralph pondera esses fatores consultando as "Leis do Projeto" (`GOALS.md`).

3.**VisÃ£o AlÃ©m dos NÃºmeros:** Um `MSE: 0.04` pode esconder um viÃ©s sistemÃ¡tico que apenas um olho humano detectaria ao ver um grÃ¡fico de resÃ­duos com padrÃ£o heterocedÃ¡stico. O Ralph utiliza **Modelos de VisÃ£o (VLM)** para "enxergar" essa geometria.

---

## ğŸ—ï¸ Arquitetura do Sistema

O sistema implementa um **Loop OODA (Observe, Orient, Decide, Act)** persistente, mantendo estado de memÃ³ria semelhante a um ambiente Jupyter Notebook.

### Diagrama de Fluxo Cognitivo

```mermaid

graph TD

    A[InÃ­cio do Ciclo] --> B{Observe}

    B -->|Leitura| C[Logs & MÃ©tricas]

    B -->|VisÃ£o| D[GrÃ¡ficos & Plots]

    B -->|MemÃ³ria| E[Estado Anterior]

  

    C & D & E --> F{Orient}

    F -->|Contexto| G[GOALS.md & Business Rules]

  

    G --> H{"Decide (LLM Brain)"}

    H -->|Caminho 1| I[Escrever Novo CÃ³digo]

    H -->|Caminho 2| J[Refatorar CÃ³digo Anterior]

    H -->|Caminho 3| K[Alterar ParÃ¢metros/Config]

  

    I & J & K --> L{"Act (Executor)"}

    L --> M[ExecuÃ§Ã£o Sandbox Python]

    M -->|Sucesso| N[Atualizar MemÃ³ria]

    M -->|Falha| O[DiagnÃ³stico de Erro]

  

    N --> A

    O --> H

```

### Componentes Core

1. **The Brain (`brain.py`):**

   * O orquestrador central baseado em LLM (GPT-4o ou Gemini 1.5 Pro).
   * ResponsÃ¡vel pelo planejamento estratÃ©gico e geraÃ§Ã£o de cÃ³digo dinÃ¢mico.
   * **Injeta contexto do projeto:** colunas reais (`state/metadata.json`), decisÃµes jÃ¡ tomadas, resumo de anÃ¡lise e **todo o conteÃºdo da pasta `context/`** nos prompts, para o agente nÃ£o partir do zero.
2. **The Vision Critic (`vision_critic.py`):**

   * MÃ³dulo especializado que recebe imagens geradas pelo cÃ³digo.
   * Utiliza *Intent Injection*: analisa o grÃ¡fico com base no que o cÃ³digo *tentou* mostrar (ex: "Verificar normalidade").
3. **The Stateful Executor (`executor.py`):**

   * MantÃ©m o namespace Python vivo entre iteraÃ§Ãµes.
   * Gerencia a persistÃªncia de objetos complexos (DataFrames, Modelos) e metadados em `state/`.

---

## ğŸ“‚ Pasta de Contexto (`context/`)

O agente **nÃ£o parte do zero**. Toda documentaÃ§Ã£o, exemplos de cÃ³digo e convenÃ§Ãµes que vocÃª colocar na pasta **`context/`** sÃ£o **lidos automaticamente** pelo Brain e injetados nas chamadas Ã  LLM.

### O que colocar em `context/`

* **DocumentaÃ§Ã£o:** regras de negÃ³cio, glossÃ¡rio, checklists em `.md` ou `.txt`.
* **Exemplos de cÃ³digo:** pipeline legado (ex.: `credit_scoring_pipeline.py`), padrÃµes de EDA.
* **ConfiguraÃ§Ãµes de referÃªncia:** `.yaml` ou `.json` que o agente deve seguir como padrÃ£o.

### Formato

* **ExtensÃµes lidas:** `.md`, `.py`, `.txt`, `.yaml`, `.json`.
* **Ordem:** alfabÃ©tica por nome de arquivo (prefixe com nÃºmeros se quiser ordem fixa: `01_objetivos.md`, `02_pipeline_exemplo.py`).
* O Brain concatena o conteÃºdo atÃ© um limite de caracteres para caber no contexto da LLM; arquivos muito grandes podem ser truncados.

### Regra de seguranÃ§a

* **NÃ£o** coloque dados sensÃ­veis ou secrets em `context/`.
* Use esta pasta para que o agente **reutilize** padrÃµes do projeto, exemplos que funcionaram e documentaÃ§Ã£o tÃ©cnica.

### Pipeline legado

O pipeline de referÃªncia estÃ¡ em `context/credit_scoring_pipeline.py`. Para executÃ¡-lo a partir da raiz do projeto:

```bash
python context/credit_scoring_pipeline.py
```

(O `cwd` deve ser a raiz do projeto para encontrar `train.parquet`, `test.parquet`, etc.)

---

## ğŸ¯ Tipos de Problema Suportados

O Ralph detecta automaticamente o tipo de problema apÃ³s a EDA:

| Tipo | DetecÃ§Ã£o | MÃ©tricas | Pipeline |
|------|----------|----------|----------|
| **ClassificaÃ§Ã£o BinÃ¡ria** | Target com 2 valores | AUC, F1, Precision, Recall | XGBoost + Threshold Opt |
| **ClassificaÃ§Ã£o Multiclasse** | Target com 3-10 valores | F1 Macro, Accuracy | XGBoost + CalibraÃ§Ã£o |
| **RegressÃ£o** | Target contÃ­nuo | RMSE, MAE, RÂ² | XGBoost + Residual Analysis |
| **Desconhecido** | NÃ£o detectado | - | Apenas EDA |

---

## ğŸš€ Ciclo de Vida de uma AnÃ¡lise

O Ralph nÃ£o segue um script prÃ©-definido. Ele constrÃ³i o script dinamicamente.

### Fase 0: InicializaÃ§Ã£o

```
1. Ler GOALS.md (objetivos)
2. Ler context/ (dados, documentaÃ§Ã£o, exemplos)
3. Carregar STATE.md (se existir)
4. Criar pasta runs/YYYYMMDD_HHMMSS/
```

### Fase 1: EDA ObrigatÃ³ria

Todo problema passa por EDA antes de qualquer modelagem:

```
01_load_data    â†’ Carregar dados, gerar metadata
02_eda_overview â†’ VisÃ£o geral (shape, tipos, memÃ³ria)
03_eda_nulls    â†’ Valores faltantes por feature
04_eda_target   â†’ DistribuiÃ§Ã£o do target (detecta tipo de problema!)
05_eda_distrib  â†’ DistribuiÃ§Ãµes das features
06_eda_corr     â†’ CorrelaÃ§Ãµes e redundÃ¢ncias
07_eda_drift    â†’ ComparaÃ§Ã£o treino vs teste (se aplicÃ¡vel)
```

### Fase 2: DetecÃ§Ã£o de Tipo e Planejamento

ApÃ³s a EDA do target, o agente:
1. Detecta automaticamente o tipo de problema
2. Gera TASK_LIST dinÃ¢mica baseada no tipo
3. Atualiza STATE.md com decisÃµes

### Fase 3: Modelagem Iterativa

O agente executa o pipeline apropriado para o tipo de problema detectado.

### Fase 4: DocumentaÃ§Ã£o Final

O agente gera report.md completo e exporta artefatos.

---

## ğŸ“‹ Ciclo de planejamento dinÃ¢mico (essÃªncia)

**Ãšnica referÃªncia fixa:** `GOALS.md`. TASK_LIST, etapas e cÃ³digo sÃ£o construÃ­dos dinamicamente.

1. **Objetivos** â†’ ler GOALS. **Contexto** â†’ context/, state/metadata.json, config.yaml. **O que jÃ¡ rodou** â†’ state/, runs/, CHANGELOG.
2. **TASK_LIST** â†’ ler e comparar com objetivos e com o que foi executado.
3. **Ajustes na TASK_LIST?** Se sim: add/remove/edit etapas; criar scripts em `notebooks/` para etapas novas. Se nÃ£o: rodar de onde parou.
4. **Rodar prÃ³xima etapa** â†’ analisar resultados (prints, relatÃ³rios, imagens). Isso altera planejamento? CÃ³digo atual? PrÃ³xima/futura/passada?
5. **Alterar passado** â†’ editar TASK_LIST e/ou cÃ³digo dos steps; rodar tudo de novo a partir da etapa alterada (`run_from_step`).
6. **Alterar atual** â†’ edit_code no step atual; rodar de novo e analisar.
7. **Alterar futuro** â†’ atualizar TASK_LIST e criar scripts se necessÃ¡rio; pode repensar o fluxo a cada anÃ¡lise. SÃ³ GOALS nÃ£o muda.

**AÃ§Ãµes do Brain:** `UPDATE_TASK_LIST` (add_steps, remove_steps, edit_steps, run_from), `RUN_FROM_STEP`, `EDIT_CODE`, `WRITE_CODE`, `RUN_STEP`. Replanejamento persistido em TASK_LIST.md (todas as sessÃµes).

---

## ğŸ› ï¸ InstalaÃ§Ã£o e Uso

### PrÃ©-requisitos

* Python 3.10+
* Chaves de API (OpenAI ou Google AI Studio)

### Setup Inicial

Comandos (Bash):

```bash
# 1. Clone o repositÃ³rio

git clone [https://github.com/seu-usuario/ralph-sabor-ds.git](https://github.com/seu-usuario/ralph-sabor-ds.git)

cd ralph-sabor-ds


# 2. Instale dependÃªncias

pip install -r requirements.txt


# 3. Configure o ambiente

cp .env.example .env

# Edite o .env com suas chaves: GEMINI_KEY=... ou OPENAI_API_KEY=...

```

### Estrutura do Projeto (v2.0)

```text
project_root/
â”œâ”€â”€ GOALS.md              # ğŸ¯ CritÃ©rios de sucesso (Ãºnica referÃªncia fixa)
â”œâ”€â”€ STATE.md              # ğŸ“Š MemÃ³ria resumida (atualizada pelo agente)
â”œâ”€â”€ TASK_LIST.md          # ğŸ“‹ Fila de tarefas (dinÃ¢mica)
â”œâ”€â”€ config.yaml           # âš™ï¸ ParÃ¢metros (Single Source of Truth)
â”œâ”€â”€ CHANGELOG.md          # ğŸ“ HistÃ³rico de experimentos
â”‚
â”œâ”€â”€ brain.py              # ğŸ§  CÃ©rebro (OODA + detecÃ§Ã£o de tipo de problema)
â”œâ”€â”€ executor.py           # â–¶ï¸ Executor stateful
â”œâ”€â”€ vision_critic.py      # ğŸ‘ï¸ AnÃ¡lise visual (Intent Injection)
â”œâ”€â”€ markdown_logger.py    # ğŸ“ Logging estruturado
â”‚
â”œâ”€â”€ context/              # ğŸ“š Contexto do projeto (agnÃ³stico)
â”‚   â”œâ”€â”€ README.md         # DocumentaÃ§Ã£o do problema
â”‚   â”œâ”€â”€ exemplos/         # CÃ³digo de referÃªncia
â”‚   â””â”€â”€ data/             # ğŸ“ DADOS DO PROJETO (READ-ONLY)
â”‚       â”œâ”€â”€ train.parquet
â”‚       â””â”€â”€ test.parquet
â”‚
â”œâ”€â”€ src/                  # âš™ï¸ MÃ³dulos Python (cÃ³digo pesado)
â”‚   â””â”€â”€ __init__.py       # ComeÃ§a vazio, agente cria sob demanda
â”‚
â”œâ”€â”€ notebooks/            # ğŸ““ Scripts gerados (chamam src/)
â”‚   â”œâ”€â”€ 01_load_data.py
â”‚   â”œâ”€â”€ 02_eda_overview.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ state/                # ğŸ’¾ Estado persistente
â”‚   â”œâ”€â”€ metadata.json     # DecisÃµes, mÃ©tricas, tipo de problema
â”‚   â””â”€â”€ step_*.pkl        # Pickles por step
â”‚
â””â”€â”€ runs/                 # ğŸ“Š Uma pasta por execuÃ§Ã£o
    â””â”€â”€ YYYYMMDD_HHMMSS/
        â”œâ”€â”€ report.md     # Log da execuÃ§Ã£o
        â””â”€â”€ *.png         # VisualizaÃ§Ãµes
```

### Executando o Agente

Para rodar o agente (ex.: Credit Scoring neste repositÃ³rio):

```bash
python brain.py --mode auto
```

Ou via controller:

```bash
python agent_controller.py
```

O Agente irÃ¡:

1. Ler os objetivos em **`GOALS.md`** e os parÃ¢metros em **`config.yaml`**.
2. Carregar contexto da pasta **`context/`** (documentaÃ§Ã£o, exemplos, pipeline legado).
3. Carregar dados de **`train.parquet`** e **`test.parquet`** (READ-ONLY; dados processados vÃ£o para **`state/`**).
4. Criar uma pasta **`runs/YYYYMMDD_HHMMSS/`** para esta execuÃ§Ã£o; todos os reports e plots da run vÃ£o para ela.
5. Para contexto, o agente lÃª apenas **`state/`** (metadata, decisÃµes) e a **run atual** (nÃ£o centenas de execuÃ§Ãµes antigas).

### MÃºltiplas execuÃ§Ãµes e histÃ³rico

* **Uma pasta por run:** Cada vez que vocÃª roda o agente (`python brain.py --mode auto`), Ã© criada **`runs/YYYYMMDD_HHMMSS/`**. Todos os plots e relatÃ³rios daquela execuÃ§Ã£o ficam nessa pasta.
* **Contexto enxuto:** O agente **nÃ£o** carrega o histÃ³rico de todas as runs antigas para decidir. Ele usa **`state/metadata.json`** (decisÃµes, mÃ©tricas, colunas) e, se existir, o resumo do Ãºltimo report **da run atual** (ou de `README_ANALISE.md`). Assim vocÃª pode rodar o fluxo inteiro do zero vÃ¡rias vezes, sem poluir o contexto com centenas de execuÃ§Ãµes.
* **HistÃ³rico preservado:** As pastas em **`runs/`** ficam guardadas para inspeÃ§Ã£o humana ou para `scripts/sync_report_to_readme.py` (que busca o .md mais recente em `runs/` e depois em `reports/`).

---

## ğŸ§  CustomizaÃ§Ã£o do Agente

Para adaptar o Ralph a novos domÃ­nios, vocÃª nÃ£o altera o cÃ³digo do orquestrador (`brain.py`, `agent_controller.py`); vocÃª altera as **regras e o contexto do projeto**:

* **`GOALS.md`:** define o que Ã© sucesso (KPIs, restriÃ§Ãµes Ã©ticas, latÃªncia).
* **`config.yaml`:** Single Source of Truth: parÃ¢metros do modelo, feature flags. Nunca hardcodar em Python.
* **`CHANGELOG.md`:** histÃ³rico imutÃ¡vel de experimentos; o agente lÃª antes de iniciar.
* **`context/`:** documentaÃ§Ã£o, exemplos e convenÃ§Ãµes lidos pelo agente (nÃ£o partir do zero).
* **`src/*.py`:** funÃ§Ãµes de domÃ­nio (ex.: PD para crÃ©dito) para o agente importar.

### Regras crÃ­ticas (`.cursorrules`)

* **Data Safety:** `train.parquet` e `test.parquet` sÃ£o **READ-ONLY**. Dados processados devem ser salvos em **`state/`** (ex.: `state/train_processed.parquet`) ou em arquivos com sufixo (ex.: `train_processed_06_feature_cleanup.parquet`).
* **Logging:** Usar **MarkdownLogger** (log, log_metric, log_plot) para escrever no report; **nÃ£o** usar `print()` para saÃ­da analÃ­tica. A anÃ¡lise das imagens com Vision Ã© feita pelo **brain** via **Vision Critic** apÃ³s cada step (scripts nÃ£o chamam Vision).
* **Erro:** Se um passo falhar, registrar em **`CHANGELOG.md`** como "FAILED" e reverter **`config.yaml`** ao Ãºltimo estado funcional; o agente prefere **edit_code** ou **rollback** a **stop**.

---

## ğŸ¤ ContribuiÃ§Ã£o

Este projeto Ã© uma exploraÃ§Ã£o de  **Agentic Engineering** . Pull requests sÃ£o bem-vindos, especialmente para:

* Novos *Critics* (ex: Analista de CÃ³digo EstÃ¡tico).
* Suporte a novas ferramentas de visualizaÃ§Ã£o.
* Melhorias na recuperaÃ§Ã£o de falhas do Executor.

---

*"Me fail English? That's unpossible!"* â€” Ralph Wiggum
